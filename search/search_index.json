{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AutoNN An AutoML (Automated machine learning) / No-Code ML Framework for beginners. Installation pip install nocode - autonn To upgrade the package pip install -- upgrade nocodenn - autonn Install all the dependencies pip install -r requirements.txt How to use? Open Terminal after installing the package Use the following command to launch the GUI autonn Attention Documentation for ANN models are yet to be uploaded, you are requested to be patient. Aplogies from our end. Thank you. Info Whole image classification API has been implemented in PyTorch , and the rest is implemented in Tensorflow & keras .","title":"Installation & Intro"},{"location":"#welcome-to-autonn","text":"An AutoML (Automated machine learning) / No-Code ML Framework for beginners.","title":"Welcome to AutoNN"},{"location":"#installation","text":"pip install nocode - autonn","title":"Installation"},{"location":"#to-upgrade-the-package","text":"pip install -- upgrade nocodenn - autonn","title":"To upgrade the package"},{"location":"#install-all-the-dependencies","text":"pip install -r requirements.txt","title":"Install all the dependencies"},{"location":"#how-to-use","text":"Open Terminal after installing the package Use the following command to launch the GUI autonn Attention Documentation for ANN models are yet to be uploaded, you are requested to be patient. Aplogies from our end. Thank you. Info Whole image classification API has been implemented in PyTorch , and the rest is implemented in Tensorflow & keras .","title":"How to use?"},{"location":"Contribution/","text":"Want to contribute? Great!!! You all are welcome. If you are a beginner then it is highly recommended that you go through Getting started with open source guidelines . Fork this repository into your GitHub account. Then clone the repository to your local machine using git clone command. Make sure you enter your user name. OR after forking you can simply press the <> Code button and copy the URL, open terminal and paste the url like this git clone <the_copied_URL> git clone https://github.com/you_user_name/AutoNN.git After step 2. Create a new branch using git checkout command. And then install all the dependencies. git checkout -b branch-name-here pip install -r requirements.txt After this you can go ahead, make some changes, and add some missing funtionality if you like. Commit your changes using the git commit command Example: git commit -m \"your message goes here\" 6. Push the changes to the remote repo using git push origin <branch-name> 7. Submit a pull request to the upstream repository with a proper message. If you are trying to fix an issue then the type of issue tag must be present. Docs Contribution. install Mk Docs pip install mkdocs-material You can check the documentation here . Goto the ../AutoNN/docs/ folder in your local repo. Folder_structure docs/ docs/ mkdocs.yml Use this command to see the dummy website at you local browser mkdocs serve . Whatever changes you make on documentation will be rendered in runtime.","title":"Contribution"},{"location":"Contribution/#want-to-contribute","text":"Great!!! You all are welcome. If you are a beginner then it is highly recommended that you go through Getting started with open source guidelines . Fork this repository into your GitHub account. Then clone the repository to your local machine using git clone command. Make sure you enter your user name. OR after forking you can simply press the <> Code button and copy the URL, open terminal and paste the url like this git clone <the_copied_URL> git clone https://github.com/you_user_name/AutoNN.git After step 2. Create a new branch using git checkout command. And then install all the dependencies. git checkout -b branch-name-here pip install -r requirements.txt After this you can go ahead, make some changes, and add some missing funtionality if you like. Commit your changes using the git commit command Example: git commit -m \"your message goes here\" 6. Push the changes to the remote repo using git push origin <branch-name> 7. Submit a pull request to the upstream repository with a proper message. If you are trying to fix an issue then the type of issue tag must be present.","title":"Want to contribute?"},{"location":"Contribution/#docs-contribution","text":"install Mk Docs pip install mkdocs-material You can check the documentation here . Goto the ../AutoNN/docs/ folder in your local repo. Folder_structure docs/ docs/ mkdocs.yml Use this command to see the dummy website at you local browser mkdocs serve . Whatever changes you make on documentation will be rendered in runtime.","title":"Docs Contribution."},{"location":"about/","text":"About This project is a result of our internship at C-DAC Center for Development of Advanced Computing, Pune under the supervision of Dr. Anil Kumar Gupta . Core Team Anish Konar : Designed the ANN models generator, and feature engineering for tabular dataset. Maintainer . Rajarshi Banerjee : Designed the GUI, automated CNN model generation, and image dataset augmentation. Maintainer . Sagnik Nayak : Automated data preprocessing, data cleaning, and feature engineering for tabular dataset. Maintainer . Other contributors Arjun Ghosh Souptik Das","title":"About"},{"location":"about/#about","text":"This project is a result of our internship at C-DAC Center for Development of Advanced Computing, Pune under the supervision of Dr. Anil Kumar Gupta .","title":"About"},{"location":"about/#core-team","text":"Anish Konar : Designed the ANN models generator, and feature engineering for tabular dataset. Maintainer . Rajarshi Banerjee : Designed the GUI, automated CNN model generation, and image dataset augmentation. Maintainer . Sagnik Nayak : Automated data preprocessing, data cleaning, and feature engineering for tabular dataset. Maintainer .","title":"Core Team"},{"location":"about/#other-contributors","text":"Arjun Ghosh Souptik Das","title":"Other contributors"},{"location":"benchmarks/","text":"Benchmarks Dataset GPU Accuracy on test set Epochs MNIST Nvidia GTX 1050 98.86% 20","title":"Benchmarks"},{"location":"benchmarks/#benchmarks","text":"Dataset GPU Accuracy on test set Epochs MNIST Nvidia GTX 1050 98.86% 20","title":"Benchmarks"},{"location":"documentation/cnn1/","text":"AutoNN.CNN.cnn_generator.CNN class CNN ( in_channels : int , numClasses : int , config : Optional [ list [ tuple ]] = None ): Parameters : in_channels : The number of channels in an image numClass : Total number of classes in a classification problem config : Generated configuration by AutoNN's CreateCNN.create_config() method required to create a CNN model Methods: summary() : Parameters : input_shape : Shape of the image torch.tensor [C, H, W] where, C = number of channels H = height of the image W = width of the image border : bool datatype , default = True | It prints lines between layers while displaying the summary of a model if set to True save() classes : List of classes image_shape : Tuple[int, int] (H,W) | dimension of the images path : Optional | path to the directory where the model is intended to be stored Note For the very first time path should be included, otherwise it will throw an InvalidPathError exception. filename : str | name of the model load() Parameters : PATH : Path to the trained model.pth . printmodel : bool datatype , default = False | print the model if set to True loadmodel : bool datatype , default = True | loads all the stored weights if set to True predict() paths : Union[list | tuple] | list of unknown images for testing or prediction","title":"class CNN"},{"location":"documentation/cnn1/#autonncnncnn_generatorcnn","text":"class CNN ( in_channels : int , numClasses : int , config : Optional [ list [ tuple ]] = None ): Parameters : in_channels : The number of channels in an image numClass : Total number of classes in a classification problem config : Generated configuration by AutoNN's CreateCNN.create_config() method required to create a CNN model","title":"AutoNN.CNN.cnn_generator.CNN"},{"location":"documentation/cnn1/#methods","text":"summary() : Parameters : input_shape : Shape of the image torch.tensor [C, H, W] where, C = number of channels H = height of the image W = width of the image border : bool datatype , default = True | It prints lines between layers while displaying the summary of a model if set to True save() classes : List of classes image_shape : Tuple[int, int] (H,W) | dimension of the images path : Optional | path to the directory where the model is intended to be stored Note For the very first time path should be included, otherwise it will throw an InvalidPathError exception. filename : str | name of the model load() Parameters : PATH : Path to the trained model.pth . printmodel : bool datatype , default = False | print the model if set to True loadmodel : bool datatype , default = True | loads all the stored weights if set to True predict() paths : Union[list | tuple] | list of unknown images for testing or prediction","title":"Methods:"},{"location":"documentation/cnn2/","text":"AutoNN.CNN.cnn_generator.CreateCNN class CreateCNN ( _size : int = 10 ): Parameters : _size : int | default = 10 | Maximum number of CNN models to be generated Methods: create_config() : This function will create configuration based on which CNN models will be generated. Parameters : min : int | minimum number of layers the gnerated CNN model can have max : int | maximum number of layers the gnerated CNN model can have Example >>> print ( CreateCNN . create_config ( 3 , 10 )) >>> [( 'conv' , 64 , 64 ), ( 'pool' , 1 , 64 ), ( 'conv' , 256 , 512 ), ( 'conv' , 64 , 128 ), ( 'conv' , 64 , 64 ), ( 'pool' , 0 , 64 )] print_all_cnn_configs() : This function will print all the CNN architectures in PyTorch Format Parameters : None print_all_architecture() This function will print all the CNN architectures generated Parameters : None get_bestCNN() Parameters: path_trainset : str | path to the image training set path_testset : str | Optional[str] | path to the image test set split_required : bool | default = False | set to true if only there is no test set batch_size : int | default = 16 | Batch size lossFn : str | default = cross-entropy | Most multiclass image classification problems use CrossEntropyLoss Info Only Cross Entropy Loss has been implemented. Since most image classification tasks uses cross entropy loss as the preferred loss function. LR : float | default = 3e4 | Learning Rate EPOCHS : int | default = 10 | number Epochs image_shape : Tuple[int,int] | default = (28,28) | dimension of the input image from the training dataset Returns : Returns a tuple containing the best CNN model generated, its configuration, and history of all models generated ( best_CNN_model, best_model_config, history_of_all_models)","title":"class CreateCNN"},{"location":"documentation/cnn2/#autonncnncnn_generatorcreatecnn","text":"class CreateCNN ( _size : int = 10 ): Parameters : _size : int | default = 10 | Maximum number of CNN models to be generated","title":"AutoNN.CNN.cnn_generator.CreateCNN"},{"location":"documentation/cnn2/#methods","text":"","title":"Methods:"},{"location":"documentation/cnn2/#create_config","text":"This function will create configuration based on which CNN models will be generated. Parameters : min : int | minimum number of layers the gnerated CNN model can have max : int | maximum number of layers the gnerated CNN model can have Example >>> print ( CreateCNN . create_config ( 3 , 10 )) >>> [( 'conv' , 64 , 64 ), ( 'pool' , 1 , 64 ), ( 'conv' , 256 , 512 ), ( 'conv' , 64 , 128 ), ( 'conv' , 64 , 64 ), ( 'pool' , 0 , 64 )]","title":"create_config() :"},{"location":"documentation/cnn2/#print_all_cnn_configs","text":"This function will print all the CNN architectures in PyTorch Format Parameters : None","title":"print_all_cnn_configs()  :"},{"location":"documentation/cnn2/#print_all_architecture","text":"This function will print all the CNN architectures generated Parameters : None","title":"print_all_architecture()"},{"location":"documentation/cnn2/#get_bestcnn","text":"Parameters: path_trainset : str | path to the image training set path_testset : str | Optional[str] | path to the image test set split_required : bool | default = False | set to true if only there is no test set batch_size : int | default = 16 | Batch size lossFn : str | default = cross-entropy | Most multiclass image classification problems use CrossEntropyLoss Info Only Cross Entropy Loss has been implemented. Since most image classification tasks uses cross entropy loss as the preferred loss function. LR : float | default = 3e4 | Learning Rate EPOCHS : int | default = 10 | number Epochs image_shape : Tuple[int,int] | default = (28,28) | dimension of the input image from the training dataset Returns : Returns a tuple containing the best CNN model generated, its configuration, and history of all models generated ( best_CNN_model, best_model_config, history_of_all_models)","title":"get_bestCNN()"},{"location":"documentation/cnn3/","text":"AutoNN.CNN.cnnBlocks cnnBlocks.SkipLayer This is basically a residual block, which is the core component of AutoNN's CNN architecture. Bunch of residual blocks are stacked together along with Pooling layers to give the full CNN architecture. Source Code class SkipLayer ( nn . Module ): def __init__ ( self , in_channels , featureMaps1 , featureMaps2 , kernel = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ): super ( SkipLayer , self ) . __init__ () self . skiplayers = nn . Sequential ( nn . Conv2d ( in_channels , featureMaps1 , kernel , stride , padding = padding ), nn . BatchNorm2d ( featureMaps1 ), nn . ReLU (), nn . Conv2d ( featureMaps1 , featureMaps2 , kernel , stride , padding = padding ), nn . BatchNorm2d ( featureMaps2 ) ) self . skip_connection = nn . Conv2d ( in_channels , featureMaps2 , kernel_size = ( 1 , 1 ), stride = stride ) self . relu = nn . ReLU () def forward ( self , x ): x0 = x . clone () x = self . skiplayers ( x ) x0 = self . skip_connection ( x0 ) x += x0 return self . relu ( x ) cnnBlocks.Pooling Pooling layers consists of both MaxPool and AvgPool , which can be selected at the time of creating an instance of the Pooling class by providing the pool_type . Source Code class Pooling ( nn . Module ): def __init__ ( self , pool_type = 'maxpool' ): super ( Pooling , self ) . __init__ () \"\"\" Args: pool_type: 'maxpool' | nn.MaxPool2d 'avgpool' | nn.AvgPool2d \"\"\" if pool_type . lower () == 'maxpool' : self . pool = nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )) else : self . pool = nn . AvgPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )) def forward ( self , x ): return self . pool ( x )","title":"CNN.codeBlocks"},{"location":"documentation/cnn3/#autonncnncnnblocks","text":"","title":"AutoNN.CNN.cnnBlocks"},{"location":"documentation/cnn3/#cnnblocksskiplayer","text":"This is basically a residual block, which is the core component of AutoNN's CNN architecture. Bunch of residual blocks are stacked together along with Pooling layers to give the full CNN architecture.","title":"cnnBlocks.SkipLayer"},{"location":"documentation/cnn3/#source-code","text":"class SkipLayer ( nn . Module ): def __init__ ( self , in_channels , featureMaps1 , featureMaps2 , kernel = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ): super ( SkipLayer , self ) . __init__ () self . skiplayers = nn . Sequential ( nn . Conv2d ( in_channels , featureMaps1 , kernel , stride , padding = padding ), nn . BatchNorm2d ( featureMaps1 ), nn . ReLU (), nn . Conv2d ( featureMaps1 , featureMaps2 , kernel , stride , padding = padding ), nn . BatchNorm2d ( featureMaps2 ) ) self . skip_connection = nn . Conv2d ( in_channels , featureMaps2 , kernel_size = ( 1 , 1 ), stride = stride ) self . relu = nn . ReLU () def forward ( self , x ): x0 = x . clone () x = self . skiplayers ( x ) x0 = self . skip_connection ( x0 ) x += x0 return self . relu ( x )","title":"Source Code"},{"location":"documentation/cnn3/#cnnblockspooling","text":"Pooling layers consists of both MaxPool and AvgPool , which can be selected at the time of creating an instance of the Pooling class by providing the pool_type .","title":"cnnBlocks.Pooling"},{"location":"documentation/cnn3/#source-code_1","text":"class Pooling ( nn . Module ): def __init__ ( self , pool_type = 'maxpool' ): super ( Pooling , self ) . __init__ () \"\"\" Args: pool_type: 'maxpool' | nn.MaxPool2d 'avgpool' | nn.AvgPool2d \"\"\" if pool_type . lower () == 'maxpool' : self . pool = nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )) else : self . pool = nn . AvgPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )) def forward ( self , x ): return self . pool ( x )","title":"Source Code"},{"location":"documentation/doc/","text":"write some class","title":"write some class"},{"location":"documentation/doc/#write-some-class","text":"","title":"write some class"},{"location":"documentation/resnet/","text":"AutoNN.CNN.models.resnet.resnet def resnet ( architecture : int = 18 , ** kwargs ) -> ResNet This function can be used to call different Resnet architectures. All resnet architectures are implemted in PyTorch Resnet num_residual_block 18 [2,2,2,2] 34 [3,4,6,3] 50 [3,4,6,3] 101 [3,4,23,3] 152 [3,8,36,3] custom any combination you like Example: To use ResNet50 with 10 output classes use: from AutoNN.CNN.models.resnet import resnet model = resnet ( architecture = 50 , num_class = 10 ) print ( model ) If you want to define your ResNet with custom number of blocks and then use: model = resnet ( architecture =- 1 , in_channels = 3 , num_residual_block = [ 3 , 4 , 6 , 3 ], num_class = 10 , block_type = 'normal' ) Arguments : architecture = -1 means you can choose the #layers of your resnet inchannels = number of input channels num_residual_blocks = number of residual blocks in each layer, [3,4,6,3] means the model has 4 layers with 1st layer containing 3 residual blocks , 2nd layer --> 4 residual blocks 3rd layer --> 6 residual blocks 4th layer --> 3 residual blocks num_class = number of classes (in this case it's 10) block_type = this has two types i) 'normal ' : has residual block with (3x3 conv, 3x3 conv) in that order ii) 'botleneck ' : has limeisual block with (1x1 conv, 3x3 conv, 1x1 conv) in that order Returns : Resnet() with desires number of layers and residual blocks Example : Custom Model model = resnet ( - 1 , num_residual_blocks = [ 2 , 1 ], num_class = 4 , block_type = 'bottleneck' ) Output ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (resnet): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downSample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downSample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=4, bias=True) )","title":"def resnet"},{"location":"documentation/resnet/#autonncnnmodelsresnetresnet","text":"def resnet ( architecture : int = 18 , ** kwargs ) -> ResNet This function can be used to call different Resnet architectures. All resnet architectures are implemted in PyTorch Resnet num_residual_block 18 [2,2,2,2] 34 [3,4,6,3] 50 [3,4,6,3] 101 [3,4,23,3] 152 [3,8,36,3] custom any combination you like Example: To use ResNet50 with 10 output classes use: from AutoNN.CNN.models.resnet import resnet model = resnet ( architecture = 50 , num_class = 10 ) print ( model ) If you want to define your ResNet with custom number of blocks and then use: model = resnet ( architecture =- 1 , in_channels = 3 , num_residual_block = [ 3 , 4 , 6 , 3 ], num_class = 10 , block_type = 'normal' ) Arguments : architecture = -1 means you can choose the #layers of your resnet inchannels = number of input channels num_residual_blocks = number of residual blocks in each layer, [3,4,6,3] means the model has 4 layers with 1st layer containing 3 residual blocks , 2nd layer --> 4 residual blocks 3rd layer --> 6 residual blocks 4th layer --> 3 residual blocks num_class = number of classes (in this case it's 10) block_type = this has two types i) 'normal ' : has residual block with (3x3 conv, 3x3 conv) in that order ii) 'botleneck ' : has limeisual block with (1x1 conv, 3x3 conv, 1x1 conv) in that order Returns : Resnet() with desires number of layers and residual blocks Example : Custom Model model = resnet ( - 1 , num_residual_blocks = [ 2 , 1 ], num_class = 4 , block_type = 'bottleneck' ) Output ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (resnet): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downSample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downSample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=4, bias=True) )","title":"AutoNN.CNN.models.resnet.resnet"},{"location":"documentation/utils1/","text":"AutoNN.CNN.utils.image_augmentation.Augment class Augment ( path ) This class will augment the image dataset using the following image operations: Rotations Horizontal Flip Vertical Flip Parameters : path : str | path to the image dataset folder Tip ''' path: provide the path to your image folder which you want to augment ../Folder/dataset/cats/x1.png ../Folder/dataset/cats/x2.png . . . ../Folder/dataset/dogs/xx1.png ../Folder/dataset/dogs/xx2.png ../Folder/dataset/dogs/xx3.png . . path = '../Folder/dataset/' ''' Method: augment() : Call this function to start augmentation Parameters : None Returns : None","title":"class Augment"},{"location":"documentation/utils1/#autonncnnutilsimage_augmentationaugment","text":"class Augment ( path ) This class will augment the image dataset using the following image operations: Rotations Horizontal Flip Vertical Flip Parameters : path : str | path to the image dataset folder Tip ''' path: provide the path to your image folder which you want to augment ../Folder/dataset/cats/x1.png ../Folder/dataset/cats/x2.png . . . ../Folder/dataset/dogs/xx1.png ../Folder/dataset/dogs/xx2.png ../Folder/dataset/dogs/xx3.png . . path = '../Folder/dataset/' '''","title":"AutoNN.CNN.utils.image_augmentation.Augment"},{"location":"documentation/utils1/#method","text":"","title":"Method:"},{"location":"documentation/utils1/#augment","text":"Call this function to start augmentation Parameters : None Returns : None","title":"augment() :"},{"location":"gui/lesson1/","text":"How to use AutoNN GUI for tabular Dataset","title":"For Tabular Dataset"},{"location":"gui/lesson1/#how-to-use-autonn-gui-for-tabular-dataset","text":"","title":"How to use AutoNN GUI for tabular Dataset"},{"location":"gui/lesson2/","text":"How to use AutoNN GUI for Image Dataset The GUI interface Steps to run GUI : Open terminal Run the following command pip install nocode-autonn autonn Buttons Open folder : Used to select the path to the training dataset Show Configs : Displays all the initial settings before training process Predict : To make predictions on selected images Load Model : To load a trained model Display Graphs : Displays the Training loss/accuracy vs Validation loss/accuracy of the generated models only after training when pressed Open Test Folder : Used to select the path to the test dataset Start Training : Starts the model training process when pressed Augment Dataset : Augments the dataset when pressed Save Trained Model : To save the model as model_name.pth file Radio Buttons Split required : Select this button IF and only IF there is no separate test dataset Split NOT required : Default selection | Select this if you have both the training and test dataset, provide the path to both datasets by clicking Open folder and Open Test Folder buttons Entry Text Boxes Learning Rate : float | Set the Learning rate for training Enter number of Channels : int | Select the number of channels in the given training image Enter number of Classes : int | Enter the total number of classes to be classified in Enter image shape : str | height x width | Should be a string and in the following format 32x32 Info The GUI will run on the main thread and all the training and other reasonably heavy computation will be carried on different threads. Danger DO NOT CLOSE THE MAIN GUI WINDOW during the training process. This is not at all recommended, wait till the training process is over. Results upon pressing Display graphs after completion of the training process Saving the model after training Select the path where you want to store the trained model To load the trained model Select Load Model Attention This demonstration was carried out in an old version of AutoNN hence the interface looks a bit different, but the functionalities are same so you needn't worry about that.","title":"For Image Dataset"},{"location":"gui/lesson2/#how-to-use-autonn-gui-for-image-dataset","text":"","title":"How to use AutoNN GUI for Image Dataset"},{"location":"gui/lesson2/#the-gui-interface","text":"Steps to run GUI : Open terminal Run the following command pip install nocode-autonn autonn","title":"The GUI interface"},{"location":"gui/lesson2/#buttons","text":"Open folder : Used to select the path to the training dataset Show Configs : Displays all the initial settings before training process Predict : To make predictions on selected images Load Model : To load a trained model Display Graphs : Displays the Training loss/accuracy vs Validation loss/accuracy of the generated models only after training when pressed Open Test Folder : Used to select the path to the test dataset Start Training : Starts the model training process when pressed Augment Dataset : Augments the dataset when pressed Save Trained Model : To save the model as model_name.pth file","title":"Buttons"},{"location":"gui/lesson2/#radio-buttons","text":"Split required : Select this button IF and only IF there is no separate test dataset Split NOT required : Default selection | Select this if you have both the training and test dataset, provide the path to both datasets by clicking Open folder and Open Test Folder buttons","title":"Radio Buttons"},{"location":"gui/lesson2/#entry-text-boxes","text":"Learning Rate : float | Set the Learning rate for training Enter number of Channels : int | Select the number of channels in the given training image Enter number of Classes : int | Enter the total number of classes to be classified in Enter image shape : str | height x width | Should be a string and in the following format 32x32 Info The GUI will run on the main thread and all the training and other reasonably heavy computation will be carried on different threads. Danger DO NOT CLOSE THE MAIN GUI WINDOW during the training process. This is not at all recommended, wait till the training process is over.","title":"Entry Text Boxes"},{"location":"gui/lesson2/#results-upon-pressing-display-graphs-after-completion-of-the-training-process","text":"","title":"Results upon pressing Display graphs after completion of the training process"},{"location":"gui/lesson2/#saving-the-model-after-training","text":"","title":"Saving the model after training"},{"location":"gui/lesson2/#select-the-path-where-you-want-to-store-the-trained-model","text":"","title":"Select the path where you want to store the trained model"},{"location":"gui/lesson2/#to-load-the-trained-model","text":"Select Load Model Attention This demonstration was carried out in an old version of AutoNN hence the interface looks a bit different, but the functionalities are same so you needn't worry about that.","title":"To load the trained model"},{"location":"tutorial/tut/","text":"Image Classification Using AutoNN AutoNN's CNN (Convolutional Neural Network) Models are built on PyTorch. Use AutoNN to classify cifar-10 Dataset 1. How to train a CNN model for image classification: Import CreateCNN from CNN's cnn_generator module Example from AutoNN.CNN.cnn_generator import CreateCNN , CNN inst = CreateCNN () model , model_config , history = inst . get_bestCNN ( path_trainset = \"E:/output/cifar10/cifar10/train\" , path_testset = \"E:/output/cifar10/cifar10/test\" , split_required = False , EPOCHS = 10 , image_shape = ( 32 , 32 )) Output \u2591\u2588\u2580\u2580\u2588 \u2588\u2591\u2591\u2588 \u2580\u2580\u2588\u2580\u2580 \u2588\u2580\u2580\u2588 \u2592\u2588\u2584\u2591\u2592\u2588 \u2592\u2588\u2584\u2591\u2592\u2588 \u2592\u2588\u2584\u2584\u2588 \u2588\u2591\u2591\u2588 \u2591\u2591\u2588\u2591\u2591 \u2588\u2591\u2591\u2588 \u2592\u2588\u2592\u2588\u2592\u2588 \u2592\u2588\u2592\u2588\u2592\u2588 \u2592\u2588\u2591\u2592\u2588 \u2591\u2580\u2580\u2580 \u2591\u2591\u2580\u2591\u2591 \u2580\u2580\u2580\u2580 \u2592\u2588\u2591\u2591\u2580\u2588 \u2592\u2588\u2591\u2591\u2580\u2588 Version: 2.0.0 An AutoML framework by Anish Konar, Arjun Ghosh, Rajarshi Banerjee, Sagnik Nayak. Default computing platform: cuda Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] # Classes: 10 Training set size: 40000 | Validation Set size: 10000 | Test Set size: 10000 Architecture search Complete..! Time Taken: 0:00:01.688582 Number of models generated: 2 Searching for the best model. Please be patient. Thank you.... Training CNN model cnn0 Epoch: 1/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:08<00:00, 36.24it/s] Training Accuracy: 35.1775 Training Loss:1.7207594780921935 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 65.11it/s] Validation Accuracy: 43.38 Validation Loss:1.5083902006149292 Epoch: 2/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.34it/s] Training Accuracy: 44.7975 Training Loss:1.483097927236557 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.63it/s] Validation Accuracy: 47.59 Validation Loss:1.4107291974067688 Epoch: 3/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.67it/s] Training Accuracy: 49.26 Training Loss:1.3825817276716232 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.44it/s] Validation Accuracy: 49.26 Validation Loss:1.3538662633895875 Epoch: 4/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 39.78it/s] Training Accuracy: 51.4975 Training Loss:1.3264493201732634 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.79it/s] Validation Accuracy: 54.59 Validation Loss:1.263453982925415 Epoch: 5/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.61it/s] Training Accuracy: 53.3775 Training Loss:1.2788944167137146 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.93it/s] Validation Accuracy: 53.4 Validation Loss:1.2734063954353332 Epoch: 6/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.67it/s] Training Accuracy: 54.59 Training Loss:1.2445036834478378 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.81it/s] Validation Accuracy: 54.01 Validation Loss:1.2412574873924256 Epoch: 7/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.66it/s] Training Accuracy: 56.045 Training Loss:1.2121670446991921 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.66it/s] Validation Accuracy: 57.49 Validation Loss:1.1828145512580872 Epoch: 8/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 39.71it/s] Training Accuracy: 56.75 Training Loss:1.1846893740534783 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.39it/s] Validation Accuracy: 58.49 Validation Loss:1.1632665138721465 Epoch: 9/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 39.73it/s] Training Accuracy: 58.13 Training Loss:1.153260654783249 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.20it/s] Validation Accuracy: 60.65 Validation Loss:1.1054361756324769 Epoch: 10/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 40.28it/s] Training Accuracy: 58.895 Training Loss:1.1318354423761368 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 73.48it/s] Validation Accuracy: 59.0 Validation Loss:1.1538037222385407 Calculating test accuracy CNN model cnn0 Test ACCuracy: 59.84 Test Loss: 1.13254158577919 ------------------------------------------------------------------------------------------------------------------------------------------------------ ______________________________________________________________________________________________________________________________________________________ Training CNN model cnn1 Epoch: 1/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 59.80it/s] Training Accuracy: 30.5625 Training Loss:1.8546679210186006 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.54it/s] Validation Accuracy: 33.79 Validation Loss:1.7818523860931397 Epoch: 2/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.42it/s] Training Accuracy: 39.33 Training Loss:1.6098018644332885 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.10it/s] Validation Accuracy: 43.24 Validation Loss:1.520786734867096 Epoch: 3/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.35it/s] Training Accuracy: 44.2325 Training Loss:1.5038440037250518 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.69it/s] Validation Accuracy: 45.07 Validation Loss:1.4924028639793396 Epoch: 4/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.33it/s] Training Accuracy: 47.06 Training Loss:1.438860204720497 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.10it/s] Validation Accuracy: 47.9 Validation Loss:1.4355408569335937 Epoch: 5/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.41it/s] Training Accuracy: 49.3625 Training Loss:1.3855291500329971 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.00it/s] Validation Accuracy: 51.42 Validation Loss:1.328598715877533 Epoch: 6/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 59.96it/s] Training Accuracy: 51.2 Training Loss:1.3459105017662047 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 66.04it/s] Validation Accuracy: 53.64 Validation Loss:1.2791677060127258 Epoch: 7/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:44<00:00, 56.65it/s] Training Accuracy: 52.4025 Training Loss:1.3105635814905168 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 69.72it/s] Validation Accuracy: 52.93 Validation Loss:1.2803085575103759 Epoch: 8/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:44<00:00, 56.75it/s] Training Accuracy: 53.41 Training Loss:1.2841510282278061 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 69.40it/s] Validation Accuracy: 55.3 Validation Loss:1.2356650713443755 Epoch: 9/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:43<00:00, 57.20it/s] Training Accuracy: 54.435 Training Loss:1.2552653106451035 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 67.85it/s] Validation Accuracy: 57.19 Validation Loss:1.1992870372772217 Epoch: 10/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:43<00:00, 57.08it/s] Training Accuracy: 55.0625 Training Loss:1.2330288346767426 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 67.08it/s] Validation Accuracy: 56.51 Validation Loss:1.2072079391002655 Calculating test accuracy CNN model cnn1 Test ACCuracy: 55.25 Test Loss: 1.2267251291751862 ------------------------------------------------------------------------------------------------------------------------------------------------------ ______________________________________________________________________________________________________________________________________________________ Best test accuracy achieved by model cnn0: 59.84 Print the model: print ( model ) Output CNN( (network): Sequential( (0): SkipLayer( (skiplayers): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) (1): SkipLayer( (skiplayers): Sequential( (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (classifier): Sequential( (0): Linear(in_features=128, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) ) ) Get list of all classes. Example: print ( inst . get_classes ) Output ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] Print Model configuration: print ( model_config ) Output [('conv', 16, 16), ('conv', 16, 128)] Print history of all training data: print ( history ) Output { 'cnn0': { 'trainloss': [1.7207594780921935, 1.483097927236557, 1.3825817276716232, 1.3264493201732634, 1.2788944167137146, 1.2445036834478378, 1.2121670446991921, 1.1846893740534783, 1.153260654783249, 1.1318354423761368], 'trainacc': [35.1775, 44.7975, 49.26, 51.4975, 53.3775, 54.59, 56.045, 56.75, 58.13, 58.895], 'valloss': [1.5083902006149292, 1.4107291974067688, 1.3538662633895875, 1.263453982925415, 1.2734063954353332, 1.2412574873924256, 1.1828145512580872, 1.1632665138721465, 1.1054361756324769, 1.1538037222385407], 'valacc': [43.38, 47.59, 49.26, 54.59, 53.4, 54.01, 57.49, 58.49, 60.65, 59.0]}, 'cnn1': { 'trainloss': [1.8546679210186006, 1.6098018644332885, 1.5038440037250518, 1.438860204720497, 1.3855291500329971, 1.3459105017662047, 1.3105635814905168, 1.2841510282278061, 1.2552653106451035, 1.2330288346767426], 'trainacc': [30.5625, 39.33, 44.2325, 47.06, 49.3625, 51.2, 52.4025, 53.41, 54.435, 55.0625], 'valloss': [1.7818523860931397, 1.520786734867096, 1.4924028639793396, 1.4355408569335937, 1.328598715877533, 1.2791677060127258, 1.2803085575103759, 1.2356650713443755, 1.1992870372772217, 1.2072079391002655], 'valacc': [33.79, 43.24, 45.07, 47.9, 51.42, 53.64, 52.93, 55.3, 57.19, 56.51] } } 2. Plot the Training loss vs Validation loss: from AutoNN.CNN.utils.EDA import plot_graph plot_graph ( history ) 3. How to save the model: model . save ( inst . get_classes , inst . get_imageshape , path = './best models/' , filename = 'mnistmodel.pth' ) 4. To check the summary of the model: model . summary ( input_shape = ( 3 , 32 , 32 )) # this method will print the keras like summary of the model This function returns a tuple of (trainable parameters, total parameters, non-trainable parameters) . See the output section below. Output Layer Output Shape Kernal Shape #params #(weights + bias) requires_grad ------------------------------------------------------------------------------------------------------------------------------------------------------ Conv2d-1 [1, 16, 32, 32] [16, 3, 3, 3] 448 (432 + 16) True True BatchNorm2d-2 [1, 16, 32, 32] [16] 32 (16 + 16) True True ReLU-3 [1, 16, 32, 32] Conv2d-4 [1, 16, 32, 32] [16, 16, 3, 3] 2320 (2304 + 16) True True BatchNorm2d-5 [1, 16, 32, 32] [16] 32 (16 + 16) True True Conv2d-6 [1, 16, 32, 32] [16, 3, 1, 1] 64 (48 + 16) True True ReLU-7 [1, 16, 32, 32] Conv2d-8 [1, 16, 32, 32] [16, 16, 3, 3] 2320 (2304 + 16) True True BatchNorm2d-9 [1, 16, 32, 32] [16] 32 (16 + 16) True True ReLU-10 [1, 16, 32, 32] Conv2d-11 [1, 128, 32, 32] [128, 16, 3, 3] 18560 (18432 + 128) True True BatchNorm2d-12 [1, 128, 32, 32] [128] 256 (128 + 128) True True Conv2d-13 [1, 128, 32, 32] [128, 16, 1, 1] 2176 (2048 + 128) True True ReLU-14 [1, 128, 32, 32] AdaptiveAvgPool2d-15 [1, 128, 1, 1] Linear-16 [1, 32] [32, 128] 4128 (4096 + 32) True True ReLU-17 [1, 32] Linear-18 [1, 10] [10, 32] 330 (320 + 10) True True ______________________________________________________________________________________________________________________________________________________ Total parameters 30,698 Total Non-Trainable parameters 0 Total Trainable parameters 30,698 (30698, 30698, 0) 5. How to load saved model: myModel = CNN ( 3 , 10 ) myModel . load ( PATH = './best models/mnistmodel.pth' , config_path = \"./best models/mnistcfg.json\" , printmodel = True ) Output Network Architecture loaded! CNN( (network): Sequential( (0): SkipLayer( (skiplayers): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) (1): SkipLayer( (skiplayers): Sequential( (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (classifier): Sequential( (0): Linear(in_features=128, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) ) ) Loading complete, your model is now ready for evaluation! 6. How to use Loaded model on new images: test_images = [ 'E:/output/cifar10/cifar10/test/bird/0012.png' , # bird 'E:/output/cifar10/cifar10/train/ship/0002.png' ] # ship myModel . predict ( paths = test_images ) Output ['bird', 'ship'] Use AutoNN for MNIST classification Open aggle Notebook","title":"Image Classification"},{"location":"tutorial/tut/#image-classification-using-autonn","text":"AutoNN's CNN (Convolutional Neural Network) Models are built on PyTorch.","title":"Image Classification Using AutoNN"},{"location":"tutorial/tut/#use-autonn-to-classify-cifar-10-dataset","text":"","title":"Use AutoNN to classify cifar-10 Dataset"},{"location":"tutorial/tut/#1-how-to-train-a-cnn-model-for-image-classification","text":"Import CreateCNN from CNN's cnn_generator module Example from AutoNN.CNN.cnn_generator import CreateCNN , CNN inst = CreateCNN () model , model_config , history = inst . get_bestCNN ( path_trainset = \"E:/output/cifar10/cifar10/train\" , path_testset = \"E:/output/cifar10/cifar10/test\" , split_required = False , EPOCHS = 10 , image_shape = ( 32 , 32 )) Output \u2591\u2588\u2580\u2580\u2588 \u2588\u2591\u2591\u2588 \u2580\u2580\u2588\u2580\u2580 \u2588\u2580\u2580\u2588 \u2592\u2588\u2584\u2591\u2592\u2588 \u2592\u2588\u2584\u2591\u2592\u2588 \u2592\u2588\u2584\u2584\u2588 \u2588\u2591\u2591\u2588 \u2591\u2591\u2588\u2591\u2591 \u2588\u2591\u2591\u2588 \u2592\u2588\u2592\u2588\u2592\u2588 \u2592\u2588\u2592\u2588\u2592\u2588 \u2592\u2588\u2591\u2592\u2588 \u2591\u2580\u2580\u2580 \u2591\u2591\u2580\u2591\u2591 \u2580\u2580\u2580\u2580 \u2592\u2588\u2591\u2591\u2580\u2588 \u2592\u2588\u2591\u2591\u2580\u2588 Version: 2.0.0 An AutoML framework by Anish Konar, Arjun Ghosh, Rajarshi Banerjee, Sagnik Nayak. Default computing platform: cuda Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] # Classes: 10 Training set size: 40000 | Validation Set size: 10000 | Test Set size: 10000 Architecture search Complete..! Time Taken: 0:00:01.688582 Number of models generated: 2 Searching for the best model. Please be patient. Thank you.... Training CNN model cnn0 Epoch: 1/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:08<00:00, 36.24it/s] Training Accuracy: 35.1775 Training Loss:1.7207594780921935 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 65.11it/s] Validation Accuracy: 43.38 Validation Loss:1.5083902006149292 Epoch: 2/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.34it/s] Training Accuracy: 44.7975 Training Loss:1.483097927236557 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.63it/s] Validation Accuracy: 47.59 Validation Loss:1.4107291974067688 Epoch: 3/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.67it/s] Training Accuracy: 49.26 Training Loss:1.3825817276716232 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.44it/s] Validation Accuracy: 49.26 Validation Loss:1.3538662633895875 Epoch: 4/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 39.78it/s] Training Accuracy: 51.4975 Training Loss:1.3264493201732634 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.79it/s] Validation Accuracy: 54.59 Validation Loss:1.263453982925415 Epoch: 5/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.61it/s] Training Accuracy: 53.3775 Training Loss:1.2788944167137146 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.93it/s] Validation Accuracy: 53.4 Validation Loss:1.2734063954353332 Epoch: 6/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.67it/s] Training Accuracy: 54.59 Training Loss:1.2445036834478378 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.81it/s] Validation Accuracy: 54.01 Validation Loss:1.2412574873924256 Epoch: 7/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:03<00:00, 39.66it/s] Training Accuracy: 56.045 Training Loss:1.2121670446991921 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.66it/s] Validation Accuracy: 57.49 Validation Loss:1.1828145512580872 Epoch: 8/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 39.71it/s] Training Accuracy: 56.75 Training Loss:1.1846893740534783 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.39it/s] Validation Accuracy: 58.49 Validation Loss:1.1632665138721465 Epoch: 9/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 39.73it/s] Training Accuracy: 58.13 Training Loss:1.153260654783249 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 70.20it/s] Validation Accuracy: 60.65 Validation Loss:1.1054361756324769 Epoch: 10/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:02<00:00, 40.28it/s] Training Accuracy: 58.895 Training Loss:1.1318354423761368 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 73.48it/s] Validation Accuracy: 59.0 Validation Loss:1.1538037222385407 Calculating test accuracy CNN model cnn0 Test ACCuracy: 59.84 Test Loss: 1.13254158577919 ------------------------------------------------------------------------------------------------------------------------------------------------------ ______________________________________________________________________________________________________________________________________________________ Training CNN model cnn1 Epoch: 1/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 59.80it/s] Training Accuracy: 30.5625 Training Loss:1.8546679210186006 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.54it/s] Validation Accuracy: 33.79 Validation Loss:1.7818523860931397 Epoch: 2/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.42it/s] Training Accuracy: 39.33 Training Loss:1.6098018644332885 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.10it/s] Validation Accuracy: 43.24 Validation Loss:1.520786734867096 Epoch: 3/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.35it/s] Training Accuracy: 44.2325 Training Loss:1.5038440037250518 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.69it/s] Validation Accuracy: 45.07 Validation Loss:1.4924028639793396 Epoch: 4/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.33it/s] Training Accuracy: 47.06 Training Loss:1.438860204720497 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.10it/s] Validation Accuracy: 47.9 Validation Loss:1.4355408569335937 Epoch: 5/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 60.41it/s] Training Accuracy: 49.3625 Training Loss:1.3855291500329971 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 76.00it/s] Validation Accuracy: 51.42 Validation Loss:1.328598715877533 Epoch: 6/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:41<00:00, 59.96it/s] Training Accuracy: 51.2 Training Loss:1.3459105017662047 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 66.04it/s] Validation Accuracy: 53.64 Validation Loss:1.2791677060127258 Epoch: 7/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:44<00:00, 56.65it/s] Training Accuracy: 52.4025 Training Loss:1.3105635814905168 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:08<00:00, 69.72it/s] Validation Accuracy: 52.93 Validation Loss:1.2803085575103759 Epoch: 8/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:44<00:00, 56.75it/s] Training Accuracy: 53.41 Training Loss:1.2841510282278061 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 69.40it/s] Validation Accuracy: 55.3 Validation Loss:1.2356650713443755 Epoch: 9/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:43<00:00, 57.20it/s] Training Accuracy: 54.435 Training Loss:1.2552653106451035 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 67.85it/s] Validation Accuracy: 57.19 Validation Loss:1.1992870372772217 Epoch: 10/10 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:43<00:00, 57.08it/s] Training Accuracy: 55.0625 Training Loss:1.2330288346767426 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [00:09<00:00, 67.08it/s] Validation Accuracy: 56.51 Validation Loss:1.2072079391002655 Calculating test accuracy CNN model cnn1 Test ACCuracy: 55.25 Test Loss: 1.2267251291751862 ------------------------------------------------------------------------------------------------------------------------------------------------------ ______________________________________________________________________________________________________________________________________________________ Best test accuracy achieved by model cnn0: 59.84","title":"1. How to train a CNN model for image classification:"},{"location":"tutorial/tut/#print-the-model","text":"print ( model ) Output CNN( (network): Sequential( (0): SkipLayer( (skiplayers): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) (1): SkipLayer( (skiplayers): Sequential( (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (classifier): Sequential( (0): Linear(in_features=128, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) ) )","title":"Print the model:"},{"location":"tutorial/tut/#get-list-of-all-classes-example","text":"print ( inst . get_classes ) Output ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']","title":"Get list of all classes. Example:"},{"location":"tutorial/tut/#print-model-configuration","text":"print ( model_config ) Output [('conv', 16, 16), ('conv', 16, 128)]","title":"Print Model configuration:"},{"location":"tutorial/tut/#print-history-of-all-training-data","text":"print ( history ) Output { 'cnn0': { 'trainloss': [1.7207594780921935, 1.483097927236557, 1.3825817276716232, 1.3264493201732634, 1.2788944167137146, 1.2445036834478378, 1.2121670446991921, 1.1846893740534783, 1.153260654783249, 1.1318354423761368], 'trainacc': [35.1775, 44.7975, 49.26, 51.4975, 53.3775, 54.59, 56.045, 56.75, 58.13, 58.895], 'valloss': [1.5083902006149292, 1.4107291974067688, 1.3538662633895875, 1.263453982925415, 1.2734063954353332, 1.2412574873924256, 1.1828145512580872, 1.1632665138721465, 1.1054361756324769, 1.1538037222385407], 'valacc': [43.38, 47.59, 49.26, 54.59, 53.4, 54.01, 57.49, 58.49, 60.65, 59.0]}, 'cnn1': { 'trainloss': [1.8546679210186006, 1.6098018644332885, 1.5038440037250518, 1.438860204720497, 1.3855291500329971, 1.3459105017662047, 1.3105635814905168, 1.2841510282278061, 1.2552653106451035, 1.2330288346767426], 'trainacc': [30.5625, 39.33, 44.2325, 47.06, 49.3625, 51.2, 52.4025, 53.41, 54.435, 55.0625], 'valloss': [1.7818523860931397, 1.520786734867096, 1.4924028639793396, 1.4355408569335937, 1.328598715877533, 1.2791677060127258, 1.2803085575103759, 1.2356650713443755, 1.1992870372772217, 1.2072079391002655], 'valacc': [33.79, 43.24, 45.07, 47.9, 51.42, 53.64, 52.93, 55.3, 57.19, 56.51] } }","title":"Print history of all training data:"},{"location":"tutorial/tut/#2-plot-the-training-loss-vs-validation-loss","text":"from AutoNN.CNN.utils.EDA import plot_graph plot_graph ( history )","title":"2. Plot the Training loss vs Validation loss:"},{"location":"tutorial/tut/#3-how-to-save-the-model","text":"model . save ( inst . get_classes , inst . get_imageshape , path = './best models/' , filename = 'mnistmodel.pth' )","title":"3. How to save the model:"},{"location":"tutorial/tut/#4-to-check-the-summary-of-the-model","text":"model . summary ( input_shape = ( 3 , 32 , 32 )) # this method will print the keras like summary of the model This function returns a tuple of (trainable parameters, total parameters, non-trainable parameters) . See the output section below. Output Layer Output Shape Kernal Shape #params #(weights + bias) requires_grad ------------------------------------------------------------------------------------------------------------------------------------------------------ Conv2d-1 [1, 16, 32, 32] [16, 3, 3, 3] 448 (432 + 16) True True BatchNorm2d-2 [1, 16, 32, 32] [16] 32 (16 + 16) True True ReLU-3 [1, 16, 32, 32] Conv2d-4 [1, 16, 32, 32] [16, 16, 3, 3] 2320 (2304 + 16) True True BatchNorm2d-5 [1, 16, 32, 32] [16] 32 (16 + 16) True True Conv2d-6 [1, 16, 32, 32] [16, 3, 1, 1] 64 (48 + 16) True True ReLU-7 [1, 16, 32, 32] Conv2d-8 [1, 16, 32, 32] [16, 16, 3, 3] 2320 (2304 + 16) True True BatchNorm2d-9 [1, 16, 32, 32] [16] 32 (16 + 16) True True ReLU-10 [1, 16, 32, 32] Conv2d-11 [1, 128, 32, 32] [128, 16, 3, 3] 18560 (18432 + 128) True True BatchNorm2d-12 [1, 128, 32, 32] [128] 256 (128 + 128) True True Conv2d-13 [1, 128, 32, 32] [128, 16, 1, 1] 2176 (2048 + 128) True True ReLU-14 [1, 128, 32, 32] AdaptiveAvgPool2d-15 [1, 128, 1, 1] Linear-16 [1, 32] [32, 128] 4128 (4096 + 32) True True ReLU-17 [1, 32] Linear-18 [1, 10] [10, 32] 330 (320 + 10) True True ______________________________________________________________________________________________________________________________________________________ Total parameters 30,698 Total Non-Trainable parameters 0 Total Trainable parameters 30,698 (30698, 30698, 0)","title":"4. To check the summary of the model:"},{"location":"tutorial/tut/#5-how-to-load-saved-model","text":"myModel = CNN ( 3 , 10 ) myModel . load ( PATH = './best models/mnistmodel.pth' , config_path = \"./best models/mnistcfg.json\" , printmodel = True ) Output Network Architecture loaded! CNN( (network): Sequential( (0): SkipLayer( (skiplayers): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) (1): SkipLayer( (skiplayers): Sequential( (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (skip_connection): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU() ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (classifier): Sequential( (0): Linear(in_features=128, out_features=32, bias=True) (1): ReLU() (2): Linear(in_features=32, out_features=10, bias=True) ) ) Loading complete, your model is now ready for evaluation!","title":"5. How to load saved model:"},{"location":"tutorial/tut/#6-how-to-use-loaded-model-on-new-images","text":"test_images = [ 'E:/output/cifar10/cifar10/test/bird/0012.png' , # bird 'E:/output/cifar10/cifar10/train/ship/0002.png' ] # ship myModel . predict ( paths = test_images ) Output ['bird', 'ship']","title":"6. How to use Loaded model on new images:"},{"location":"tutorial/tut/#use-autonn-for-mnist-classification","text":"Open aggle Notebook","title":" Use AutoNN for MNIST classification"},{"location":"tutorial/tut1/","text":"ANN models","title":"Handle Tabular Dataset"},{"location":"tutorial/tut1/#ann-models","text":"","title":"ANN models"}]}