{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samba\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Samba\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torchvision as tv\n",
    "from torch.optim import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_features=64,out_features=64,stride=[1,1],down_sample=False):\n",
    "        super(BasicBlock,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_features,out_features,3,stride[0],padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_features)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv2 = nn.Conv2d(out_features,out_features,3,stride[1],padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_features)\n",
    "\n",
    "        self.down_sample = down_sample\n",
    "        if down_sample:\n",
    "            self.downsample = nn.Sequential(\n",
    "                    nn.Conv2d(in_features,out_features,1,2,bias=False),\n",
    "                    nn.BatchNorm2d(out_features)\n",
    "                )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0=x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.down_sample:\n",
    "            x0 = self.downsample(x0)\n",
    "        x = x + x0\n",
    "        x= self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,inFeatures=64,outFeatures=64,kSize=[1,3,1],stride=[1,2,1],\n",
    "    dn_sample=False,dnSample_stride=1) -> None:\n",
    "        super(Bottleneck,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(inFeatures,outFeatures,kSize[0],stride[0],bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outFeatures)\n",
    "        self.conv2 = nn.Conv2d(outFeatures,outFeatures,kSize[1],stride[1],padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outFeatures)\n",
    "        self.conv3 = nn.Conv2d(outFeatures,outFeatures*4,kSize[2],stride[2],bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(outFeatures*4)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "\n",
    "        self.ds = dn_sample\n",
    "        if dn_sample:\n",
    "            self.downSample = nn.Sequential(\n",
    "                nn.Conv2d(inFeatures,outFeatures*4,1,stride=dnSample_stride,bias=False),\n",
    "                nn.BatchNorm2d(outFeatures*4)            \n",
    "            )\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x0 = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        if self.ds:\n",
    "            x0 = self.downSample(x0)\n",
    "        x = x+x0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,in_channels=3,num_residual_block=[3,4,6,3],num_class=10,block_type='normal'):\n",
    "        super(ResNet,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,64,7,2,3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.maxpool = nn.MaxPool2d(3,2,1)\n",
    "\n",
    "        if block_type.lower() == 'bottleneck':    \n",
    "            self.resnet,outchannels = self.__bottlenecks(num_residual_block)\n",
    "        else:\n",
    "            self.resnet,outchannels = self.__layers(num_residual_block)\n",
    "    \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(in_features=outchannels,out_features=num_class,bias=True)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def __layers(self,num_residual_block):\n",
    "        layer=[]\n",
    "        layer += [BasicBlock()]*2\n",
    "        inchannels=64\n",
    "        for numOFlayers in num_residual_block:\n",
    "            stride = [2,1]\n",
    "            downsample=True\n",
    "            outchannels = inchannels*2\n",
    "            for _ in range(numOFlayers):\n",
    "                layer.append(BasicBlock(inchannels,outchannels,stride,down_sample=downsample))\n",
    "                inchannels = outchannels\n",
    "                downsample = False \n",
    "                stride=[1,1]\n",
    "            \n",
    "        return nn.Sequential(*layer),outchannels\n",
    "\n",
    "    \n",
    "    def __bottlenecks(self,numres):\n",
    "        ''' \n",
    "        [3,4,6,3]\n",
    "        '''\n",
    "        layer=[]\n",
    "        \n",
    "        stride = [1,1,1]\n",
    "        dnStride=1\n",
    "        inchan = 64\n",
    "        for i,numOFlayers in enumerate(numres):\n",
    "            dn_sample = True\n",
    "            outchan = 64*(2**i)\n",
    "\n",
    "            for _ in range(numOFlayers):\n",
    "                layer+=[ \n",
    "                    Bottleneck(inchan,outchan,stride=stride,\n",
    "                    dn_sample=dn_sample,dnSample_stride=dnStride)\n",
    "                ]\n",
    "                inchan = outchan*4\n",
    "                dn_sample = False\n",
    "                stride = [1,1,1]   \n",
    "            dn_sample=True \n",
    "            stride = [1,2,1]\n",
    "            dnStride=2\n",
    "            \n",
    "\n",
    "        return nn.Sequential(*layer),inchan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (resnet): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (12): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (13): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (14): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (15): BasicBlock(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): BasicBlock(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (17): BasicBlock(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(num_class=1000)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    net = ResNet(num_class=1000,block_type='Bottleneck')\n",
    "    y = net(torch.randn(4, 3, 224, 224)).to(\"cuda\")\n",
    "    print(y.size())\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]           4,096\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-12        [-1, 256, 128, 128]             512\n",
      "           Conv2d-13        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-14        [-1, 256, 128, 128]             512\n",
      "             ReLU-15        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-16        [-1, 256, 128, 128]               0\n",
      "           Conv2d-17         [-1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-18         [-1, 64, 128, 128]             128\n",
      "             ReLU-19         [-1, 64, 128, 128]               0\n",
      "           Conv2d-20         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-21         [-1, 64, 128, 128]             128\n",
      "             ReLU-22         [-1, 64, 128, 128]               0\n",
      "           Conv2d-23        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-24        [-1, 256, 128, 128]             512\n",
      "             ReLU-25        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-26        [-1, 256, 128, 128]               0\n",
      "           Conv2d-27         [-1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-28         [-1, 64, 128, 128]             128\n",
      "             ReLU-29         [-1, 64, 128, 128]               0\n",
      "           Conv2d-30         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-31         [-1, 64, 128, 128]             128\n",
      "             ReLU-32         [-1, 64, 128, 128]               0\n",
      "           Conv2d-33        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-34        [-1, 256, 128, 128]             512\n",
      "             ReLU-35        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-36        [-1, 256, 128, 128]               0\n",
      "           Conv2d-37        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-38        [-1, 128, 128, 128]             256\n",
      "             ReLU-39        [-1, 128, 128, 128]               0\n",
      "           Conv2d-40          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 64, 64]             256\n",
      "             ReLU-42          [-1, 128, 64, 64]               0\n",
      "           Conv2d-43          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 64, 64]           1,024\n",
      "           Conv2d-45          [-1, 512, 64, 64]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-47          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-48          [-1, 512, 64, 64]               0\n",
      "           Conv2d-49          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 64, 64]             256\n",
      "             ReLU-51          [-1, 128, 64, 64]               0\n",
      "           Conv2d-52          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 64, 64]             256\n",
      "             ReLU-54          [-1, 128, 64, 64]               0\n",
      "           Conv2d-55          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-57          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-58          [-1, 512, 64, 64]               0\n",
      "           Conv2d-59          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 64, 64]             256\n",
      "             ReLU-61          [-1, 128, 64, 64]               0\n",
      "           Conv2d-62          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 64, 64]             256\n",
      "             ReLU-64          [-1, 128, 64, 64]               0\n",
      "           Conv2d-65          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-67          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-68          [-1, 512, 64, 64]               0\n",
      "           Conv2d-69          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 64, 64]             256\n",
      "             ReLU-71          [-1, 128, 64, 64]               0\n",
      "           Conv2d-72          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 64, 64]             256\n",
      "             ReLU-74          [-1, 128, 64, 64]               0\n",
      "           Conv2d-75          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-77          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-78          [-1, 512, 64, 64]               0\n",
      "           Conv2d-79          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-80          [-1, 128, 64, 64]             256\n",
      "             ReLU-81          [-1, 128, 64, 64]               0\n",
      "           Conv2d-82          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 64, 64]             256\n",
      "             ReLU-84          [-1, 128, 64, 64]               0\n",
      "           Conv2d-85          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-87          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-88          [-1, 512, 64, 64]               0\n",
      "           Conv2d-89          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-90          [-1, 128, 64, 64]             256\n",
      "             ReLU-91          [-1, 128, 64, 64]               0\n",
      "           Conv2d-92          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-93          [-1, 128, 64, 64]             256\n",
      "             ReLU-94          [-1, 128, 64, 64]               0\n",
      "           Conv2d-95          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-96          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-97          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-98          [-1, 512, 64, 64]               0\n",
      "           Conv2d-99          [-1, 128, 64, 64]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 64, 64]             256\n",
      "            ReLU-101          [-1, 128, 64, 64]               0\n",
      "          Conv2d-102          [-1, 128, 64, 64]         147,456\n",
      "     BatchNorm2d-103          [-1, 128, 64, 64]             256\n",
      "            ReLU-104          [-1, 128, 64, 64]               0\n",
      "          Conv2d-105          [-1, 512, 64, 64]          65,536\n",
      "     BatchNorm2d-106          [-1, 512, 64, 64]           1,024\n",
      "            ReLU-107          [-1, 512, 64, 64]               0\n",
      "      Bottleneck-108          [-1, 512, 64, 64]               0\n",
      "          Conv2d-109          [-1, 128, 64, 64]          65,536\n",
      "     BatchNorm2d-110          [-1, 128, 64, 64]             256\n",
      "            ReLU-111          [-1, 128, 64, 64]               0\n",
      "          Conv2d-112          [-1, 128, 64, 64]         147,456\n",
      "     BatchNorm2d-113          [-1, 128, 64, 64]             256\n",
      "            ReLU-114          [-1, 128, 64, 64]               0\n",
      "          Conv2d-115          [-1, 512, 64, 64]          65,536\n",
      "     BatchNorm2d-116          [-1, 512, 64, 64]           1,024\n",
      "            ReLU-117          [-1, 512, 64, 64]               0\n",
      "      Bottleneck-118          [-1, 512, 64, 64]               0\n",
      "          Conv2d-119          [-1, 256, 64, 64]         131,072\n",
      "     BatchNorm2d-120          [-1, 256, 64, 64]             512\n",
      "            ReLU-121          [-1, 256, 64, 64]               0\n",
      "          Conv2d-122          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 32, 32]             512\n",
      "            ReLU-124          [-1, 256, 32, 32]               0\n",
      "          Conv2d-125         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 32, 32]           2,048\n",
      "          Conv2d-127         [-1, 1024, 32, 32]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-129         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-130         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-131          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 32, 32]             512\n",
      "            ReLU-133          [-1, 256, 32, 32]               0\n",
      "          Conv2d-134          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 32, 32]             512\n",
      "            ReLU-136          [-1, 256, 32, 32]               0\n",
      "          Conv2d-137         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-139         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-140         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-141          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 32, 32]             512\n",
      "            ReLU-143          [-1, 256, 32, 32]               0\n",
      "          Conv2d-144          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 32, 32]             512\n",
      "            ReLU-146          [-1, 256, 32, 32]               0\n",
      "          Conv2d-147         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-149         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-150         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-151          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 32, 32]             512\n",
      "            ReLU-153          [-1, 256, 32, 32]               0\n",
      "          Conv2d-154          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 32, 32]             512\n",
      "            ReLU-156          [-1, 256, 32, 32]               0\n",
      "          Conv2d-157         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-159         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-160         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-161          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 32, 32]             512\n",
      "            ReLU-163          [-1, 256, 32, 32]               0\n",
      "          Conv2d-164          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 32, 32]             512\n",
      "            ReLU-166          [-1, 256, 32, 32]               0\n",
      "          Conv2d-167         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-169         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-170         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-171          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 32, 32]             512\n",
      "            ReLU-173          [-1, 256, 32, 32]               0\n",
      "          Conv2d-174          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 32, 32]             512\n",
      "            ReLU-176          [-1, 256, 32, 32]               0\n",
      "          Conv2d-177         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-179         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-180         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-181          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 32, 32]             512\n",
      "            ReLU-183          [-1, 256, 32, 32]               0\n",
      "          Conv2d-184          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 32, 32]             512\n",
      "            ReLU-186          [-1, 256, 32, 32]               0\n",
      "          Conv2d-187         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-189         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-190         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-191          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 32, 32]             512\n",
      "            ReLU-193          [-1, 256, 32, 32]               0\n",
      "          Conv2d-194          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 32, 32]             512\n",
      "            ReLU-196          [-1, 256, 32, 32]               0\n",
      "          Conv2d-197         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-199         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-200         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-201          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 32, 32]             512\n",
      "            ReLU-203          [-1, 256, 32, 32]               0\n",
      "          Conv2d-204          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 32, 32]             512\n",
      "            ReLU-206          [-1, 256, 32, 32]               0\n",
      "          Conv2d-207         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-209         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-210         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-211          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 32, 32]             512\n",
      "            ReLU-213          [-1, 256, 32, 32]               0\n",
      "          Conv2d-214          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 32, 32]             512\n",
      "            ReLU-216          [-1, 256, 32, 32]               0\n",
      "          Conv2d-217         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-219         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-220         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-221          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 32, 32]             512\n",
      "            ReLU-223          [-1, 256, 32, 32]               0\n",
      "          Conv2d-224          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 32, 32]             512\n",
      "            ReLU-226          [-1, 256, 32, 32]               0\n",
      "          Conv2d-227         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-229         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-230         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-231          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 32, 32]             512\n",
      "            ReLU-233          [-1, 256, 32, 32]               0\n",
      "          Conv2d-234          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 32, 32]             512\n",
      "            ReLU-236          [-1, 256, 32, 32]               0\n",
      "          Conv2d-237         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-239         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-240         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-241          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 32, 32]             512\n",
      "            ReLU-243          [-1, 256, 32, 32]               0\n",
      "          Conv2d-244          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 32, 32]             512\n",
      "            ReLU-246          [-1, 256, 32, 32]               0\n",
      "          Conv2d-247         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-249         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-250         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-251          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 32, 32]             512\n",
      "            ReLU-253          [-1, 256, 32, 32]               0\n",
      "          Conv2d-254          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 32, 32]             512\n",
      "            ReLU-256          [-1, 256, 32, 32]               0\n",
      "          Conv2d-257         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-259         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-260         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-261          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 32, 32]             512\n",
      "            ReLU-263          [-1, 256, 32, 32]               0\n",
      "          Conv2d-264          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 32, 32]             512\n",
      "            ReLU-266          [-1, 256, 32, 32]               0\n",
      "          Conv2d-267         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-269         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-270         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-271          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 32, 32]             512\n",
      "            ReLU-273          [-1, 256, 32, 32]               0\n",
      "          Conv2d-274          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 32, 32]             512\n",
      "            ReLU-276          [-1, 256, 32, 32]               0\n",
      "          Conv2d-277         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-279         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-280         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-281          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 32, 32]             512\n",
      "            ReLU-283          [-1, 256, 32, 32]               0\n",
      "          Conv2d-284          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 32, 32]             512\n",
      "            ReLU-286          [-1, 256, 32, 32]               0\n",
      "          Conv2d-287         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-289         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-290         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-291          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 32, 32]             512\n",
      "            ReLU-293          [-1, 256, 32, 32]               0\n",
      "          Conv2d-294          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 32, 32]             512\n",
      "            ReLU-296          [-1, 256, 32, 32]               0\n",
      "          Conv2d-297         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-299         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-300         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-301          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 32, 32]             512\n",
      "            ReLU-303          [-1, 256, 32, 32]               0\n",
      "          Conv2d-304          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 32, 32]             512\n",
      "            ReLU-306          [-1, 256, 32, 32]               0\n",
      "          Conv2d-307         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-309         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-310         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-311          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 32, 32]             512\n",
      "            ReLU-313          [-1, 256, 32, 32]               0\n",
      "          Conv2d-314          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 32, 32]             512\n",
      "            ReLU-316          [-1, 256, 32, 32]               0\n",
      "          Conv2d-317         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-318         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-319         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-320         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-321          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-322          [-1, 256, 32, 32]             512\n",
      "            ReLU-323          [-1, 256, 32, 32]               0\n",
      "          Conv2d-324          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-325          [-1, 256, 32, 32]             512\n",
      "            ReLU-326          [-1, 256, 32, 32]               0\n",
      "          Conv2d-327         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-328         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-329         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-330         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-331          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-332          [-1, 256, 32, 32]             512\n",
      "            ReLU-333          [-1, 256, 32, 32]               0\n",
      "          Conv2d-334          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-335          [-1, 256, 32, 32]             512\n",
      "            ReLU-336          [-1, 256, 32, 32]               0\n",
      "          Conv2d-337         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-338         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-339         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-340         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-341          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-342          [-1, 256, 32, 32]             512\n",
      "            ReLU-343          [-1, 256, 32, 32]               0\n",
      "          Conv2d-344          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-345          [-1, 256, 32, 32]             512\n",
      "            ReLU-346          [-1, 256, 32, 32]               0\n",
      "          Conv2d-347         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-348         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-349         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-350         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-351          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-352          [-1, 256, 32, 32]             512\n",
      "            ReLU-353          [-1, 256, 32, 32]               0\n",
      "          Conv2d-354          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-355          [-1, 256, 32, 32]             512\n",
      "            ReLU-356          [-1, 256, 32, 32]               0\n",
      "          Conv2d-357         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-358         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-359         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-360         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-361          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-362          [-1, 256, 32, 32]             512\n",
      "            ReLU-363          [-1, 256, 32, 32]               0\n",
      "          Conv2d-364          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-365          [-1, 256, 32, 32]             512\n",
      "            ReLU-366          [-1, 256, 32, 32]               0\n",
      "          Conv2d-367         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-369         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-370         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-371          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-372          [-1, 256, 32, 32]             512\n",
      "            ReLU-373          [-1, 256, 32, 32]               0\n",
      "          Conv2d-374          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-375          [-1, 256, 32, 32]             512\n",
      "            ReLU-376          [-1, 256, 32, 32]               0\n",
      "          Conv2d-377         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-378         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-379         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-380         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-381          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-382          [-1, 256, 32, 32]             512\n",
      "            ReLU-383          [-1, 256, 32, 32]               0\n",
      "          Conv2d-384          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-385          [-1, 256, 32, 32]             512\n",
      "            ReLU-386          [-1, 256, 32, 32]               0\n",
      "          Conv2d-387         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-388         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-389         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-390         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-391          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-392          [-1, 256, 32, 32]             512\n",
      "            ReLU-393          [-1, 256, 32, 32]               0\n",
      "          Conv2d-394          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-395          [-1, 256, 32, 32]             512\n",
      "            ReLU-396          [-1, 256, 32, 32]               0\n",
      "          Conv2d-397         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-398         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-399         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-400         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-401          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-402          [-1, 256, 32, 32]             512\n",
      "            ReLU-403          [-1, 256, 32, 32]               0\n",
      "          Conv2d-404          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-405          [-1, 256, 32, 32]             512\n",
      "            ReLU-406          [-1, 256, 32, 32]               0\n",
      "          Conv2d-407         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-408         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-409         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-410         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-411          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-412          [-1, 256, 32, 32]             512\n",
      "            ReLU-413          [-1, 256, 32, 32]               0\n",
      "          Conv2d-414          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-415          [-1, 256, 32, 32]             512\n",
      "            ReLU-416          [-1, 256, 32, 32]               0\n",
      "          Conv2d-417         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-418         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-419         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-420         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-421          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-422          [-1, 256, 32, 32]             512\n",
      "            ReLU-423          [-1, 256, 32, 32]               0\n",
      "          Conv2d-424          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-425          [-1, 256, 32, 32]             512\n",
      "            ReLU-426          [-1, 256, 32, 32]               0\n",
      "          Conv2d-427         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-428         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-429         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-430         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-431          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-432          [-1, 256, 32, 32]             512\n",
      "            ReLU-433          [-1, 256, 32, 32]               0\n",
      "          Conv2d-434          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-435          [-1, 256, 32, 32]             512\n",
      "            ReLU-436          [-1, 256, 32, 32]               0\n",
      "          Conv2d-437         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-438         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-439         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-440         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-441          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-442          [-1, 256, 32, 32]             512\n",
      "            ReLU-443          [-1, 256, 32, 32]               0\n",
      "          Conv2d-444          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-445          [-1, 256, 32, 32]             512\n",
      "            ReLU-446          [-1, 256, 32, 32]               0\n",
      "          Conv2d-447         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-448         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-449         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-450         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-451          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-452          [-1, 256, 32, 32]             512\n",
      "            ReLU-453          [-1, 256, 32, 32]               0\n",
      "          Conv2d-454          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-455          [-1, 256, 32, 32]             512\n",
      "            ReLU-456          [-1, 256, 32, 32]               0\n",
      "          Conv2d-457         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-458         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-459         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-460         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-461          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-462          [-1, 256, 32, 32]             512\n",
      "            ReLU-463          [-1, 256, 32, 32]               0\n",
      "          Conv2d-464          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-465          [-1, 256, 32, 32]             512\n",
      "            ReLU-466          [-1, 256, 32, 32]               0\n",
      "          Conv2d-467         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-468         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-469         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-470         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-471          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-472          [-1, 256, 32, 32]             512\n",
      "            ReLU-473          [-1, 256, 32, 32]               0\n",
      "          Conv2d-474          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-475          [-1, 256, 32, 32]             512\n",
      "            ReLU-476          [-1, 256, 32, 32]               0\n",
      "          Conv2d-477         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-478         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-479         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-480         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-481          [-1, 512, 32, 32]         524,288\n",
      "     BatchNorm2d-482          [-1, 512, 32, 32]           1,024\n",
      "            ReLU-483          [-1, 512, 32, 32]               0\n",
      "          Conv2d-484          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-485          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-486          [-1, 512, 16, 16]               0\n",
      "          Conv2d-487         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-488         [-1, 2048, 16, 16]           4,096\n",
      "          Conv2d-489         [-1, 2048, 16, 16]       2,097,152\n",
      "     BatchNorm2d-490         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-491         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-492         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-493          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-494          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-495          [-1, 512, 16, 16]               0\n",
      "          Conv2d-496          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-497          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-498          [-1, 512, 16, 16]               0\n",
      "          Conv2d-499         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-500         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-501         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-502         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-503          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-504          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-505          [-1, 512, 16, 16]               0\n",
      "          Conv2d-506          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-507          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-508          [-1, 512, 16, 16]               0\n",
      "          Conv2d-509         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-510         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-511         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-512         [-1, 2048, 16, 16]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 60,192,808\n",
      "Trainable params: 60,192,808\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 3169.02\n",
      "Params size (MB): 229.62\n",
      "Estimated Total Size (MB): 3401.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary \n",
    "model= tv.models.resnet152(False)\n",
    "summary(model.cuda(),(3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c74d99681ad1a2c591eba718ee02a1ecc6e61532130e8f76172a42e8769d33f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
